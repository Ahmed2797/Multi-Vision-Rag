# ğŸš€ VisionRAG

**VisionRAG** is an image-visible **Multimodal Retrieval-Augmented Generation (RAG)** system that can retrieve and display **text, tables, and images** from documents using vision-aware semantic search.

Unlike traditional RAG systems that only work with text, VisionRAG understands **what an image represents**, retrieves it when relevant, and displays the actual image alongside the generated answer.

---

## âœ¨ Key Features

* ğŸ” **Multimodal Retrieval** â€“ Search across text, tables, and images
* ğŸ–¼ï¸ **Image-Visible RAG** â€“ Retrieved images are displayed, not just described
* ğŸ§  **Vision-Aware Embeddings** â€“ Images are indexed using semantic summaries
* ğŸ“„ **PDF Understanding** â€“ Supports complex PDFs with tables and figures
* âš™ï¸ **Production-Style Architecture** â€“ Multi-vector retrieval + docstore

---

## ğŸ§  How VisionRAG Works

1. **Document Ingestion**

   * PDFs are partitioned into text, tables, and images using *Unstructured*

2. **Image Understanding**

   * Each image is summarized using a Vision LLM
   * The summary is embedded for semantic search

3. **Storage Strategy**

   * **Vector Store** â†’ Text summaries (searchable)
   * **Docstore** â†’ Original content (text, tables, image base64)

4. **Query Flow**

   * User query â†’ semantic search over summaries
   * Matching documents retrieved
   * Images are rendered and answers are generated using LLM

---

## ğŸ—ï¸ Architecture Overview

```

PDF Documents
     â†“
Partition (Text | Table | Image)
     â†“
Image â†’ Vision Summary â†’ Embedding
Text  â†’ Text Summary  â†’ Embedding
     â†“
Vector Store (Search)
     â†“
Docstore (Original Content)
     â†“
LLM + UI (Text + Image Response)
```

---

## ğŸ› ï¸ Tech Stack

* **Python**
* **LangChain**
* **Unstructured** (PDF parsing)
* **OpenAI GPT-4.1-nano** (Text & Vision)
* **OpenAI Embeddings**
* **ChromaDB** (Vector Store)
* **MultiVectorRetriever**

---

## ğŸ“¦ Installation

```bash
pip install langchain unstructured chromadb openai
```

Make sure you have your OpenAI API key set:

```bash
export OPENAI_API_KEY="your_api_key_here"
```

---

## â–¶ï¸ Usage Example

```python
question = "Explain the multi-head attention diagram"
answer, context = query_rag(question)

# Display retrieved images
show_images(context["images"])

print(answer)
```

---

## ğŸ¯ Example Queries

* "Explain the attention mechanism shown in the diagram"
* "What does the transformer architecture image describe?"
* "Summarize the table comparing attention heads"

---

## ğŸ’¡ Why VisionRAG?

Traditional RAG systems:

* âŒ Cannot retrieve images
* âŒ Lose visual context

VisionRAG:

* âœ… Retrieves image semantics
* âœ… Displays the actual image
* âœ… Enables real multimodal reasoning

---

## ğŸ§ª Use Cases

* Research paper analysis
* Technical documentation QA
* Educational content understanding
* Multimodal knowledge assistants

---

## ğŸ“Œ Future Improvements

* Streamlit / Web UI
* Citation and page-number tracking
* Reranking for higher accuracy
* Support for audio & video

---

## ğŸ‘¨â€ğŸ’» Author

**Ahmed2797**
AI & ML Enthusiast | Multimodal Systems Learner

---

## â­ Acknowledgements

* LangChain
* OpenAI
* Unstructured

---

If you found this project useful, feel free to â­ the repository!
